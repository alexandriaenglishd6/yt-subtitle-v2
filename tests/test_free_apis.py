"""
æµ‹è¯•å…è´¹ APIï¼ˆæ— éœ€ API Key å’Œä»˜è´¹ï¼‰
1. Google Translateï¼ˆå…è´¹ç‰ˆï¼‰
2. æœ¬åœ°æ¨¡åž‹ï¼ˆOllama/LM Studioï¼‰
"""
import sys
import pytest
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from config.manager import AIConfig
from core.ai_providers import create_llm_client
from core.llm_client import LLMException, LLMErrorType


def test_google_translate():
    """æµ‹è¯• Google Translateï¼ˆå…è´¹ç‰ˆï¼‰"""
    print("=" * 60)
    print("æµ‹è¯• 1: Google Translateï¼ˆå…è´¹ç‰ˆï¼‰")
    print("=" * 60)
    
    try:
        # åˆ›å»ºé…ç½®ï¼ˆä¸éœ€è¦ API Keyï¼‰
        config = AIConfig(
            provider="google_translate",
            model="google_translate_free",
            base_url=None,
            timeout_seconds=30,
            max_retries=2,
            max_concurrency=5,
            api_keys={}  # ä¸éœ€è¦ API Key
        )
        
        print(f"âœ“ é…ç½®åˆ›å»ºæˆåŠŸ")
        print(f"  Provider: {config.provider}")
        print(f"  Model: {config.model}")
        print()
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        print("æ­£åœ¨åˆ›å»ºå®¢æˆ·ç«¯...")
        client = create_llm_client(config)
        print(f"âœ“ å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        print(f"  Supports Vision: {client.supports_vision}")
        print(f"  Max Input Tokens: {client.max_input_tokens}")
        print(f"  Max Concurrency: {client.max_concurrency}")
        print()
        
        # æµ‹è¯•ç¿»è¯‘
        print("æ­£åœ¨æµ‹è¯•ç¿»è¯‘...")
        test_prompt = """è¯·å°†ä»¥ä¸‹å­—å¹•ä»Ž English ç¿»è¯‘æˆ ä¸­æ–‡ã€‚

å­—å¹•å†…å®¹ï¼š
1
00:00:01,000 --> 00:00:03,000
Hello world

2
00:00:04,000 --> 00:00:06,000
This is a test

è¯·ç›´æŽ¥è¿”å›žç¿»è¯‘åŽçš„å­—å¹•å†…å®¹ï¼Œä¿æŒ SRT æ ¼å¼ã€‚"""
        
        result = client.generate(
            prompt=test_prompt,
            max_tokens=100
        )
        
        print("âœ“ ç¿»è¯‘æˆåŠŸï¼")
        print(f"  Provider: {result.provider}")
        print(f"  Model: {result.model}")
        print()
        print("ç¿»è¯‘ç»“æžœï¼š")
        print(result.text)
        print()
        
        assert result.text, "ç¿»è¯‘ç»“æžœä¸åº”ä¸ºç©º"
        
    except ImportError as e:
        pytest.skip(f"ä¾èµ–åº“æœªå®‰è£…: {e}")
    except LLMException as e:
        pytest.skip(f"ç¿»è¯‘å¤±è´¥: {e}")
    except Exception as e:
        pytest.fail(f"æœªçŸ¥é”™è¯¯: {e}")


def test_local_model():
    """æµ‹è¯•æœ¬åœ°æ¨¡åž‹ï¼ˆOllama/LM Studioï¼‰"""
    print("=" * 60)
    print("æµ‹è¯• 2: æœ¬åœ°æ¨¡åž‹ï¼ˆOllama/LM Studioï¼‰")
    print("=" * 60)
    
    # æ£€æŸ¥å¸¸è§çš„æœ¬åœ°æ¨¡åž‹åœ°å€
    local_urls = [
        ("Ollama (é»˜è®¤)", "http://localhost:11434/v1"),
        ("LM Studio (é»˜è®¤)", "http://localhost:1234/v1"),
    ]
    
    success = False
    
    for name, base_url in local_urls:
        print(f"\nå°è¯•è¿žæŽ¥: {name} ({base_url})")
        
        try:
            # åˆ›å»ºé…ç½®
            config = AIConfig(
                provider="ollama",  # ä¼šè‡ªåŠ¨ä½¿ç”¨ LocalModelClient
                model="llama3.2:3b",  # ä½¿ç”¨ä¸€ä¸ªå¸¸è§çš„æ¨¡åž‹
                base_url=base_url,
                timeout_seconds=300,
                max_retries=2,
                max_concurrency=3,
                api_keys={"openai": "ollama"}  # æœ¬åœ°æ¨¡åž‹ä¸éœ€è¦çœŸå®žçš„ API Key
            )
            
            print(f"âœ“ é…ç½®åˆ›å»ºæˆåŠŸ")
            print(f"  Provider: {config.provider}")
            print(f"  Model: {config.model}")
            print(f"  Base URL: {config.base_url}")
            print()
            
            # åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆä¼šè¿›è¡Œå¿ƒè·³æ£€æµ‹ï¼‰
            print("æ­£åœ¨åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆä¼šè¿›è¡Œå¿ƒè·³æ£€æµ‹ï¼‰...")
            client = create_llm_client(config)
            print(f"âœ“ å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
            print(f"  ç±»åž‹: {type(client).__name__}")
            print()
            
            # æµ‹è¯•ç”Ÿæˆï¼ˆä¼šè¿›è¡Œé¢„çƒ­ï¼‰
            print("æ­£åœ¨æµ‹è¯•ç”Ÿæˆï¼ˆé¦–æ¬¡è°ƒç”¨ä¼šè¿›è¡Œé¢„çƒ­ï¼‰...")
            result = client.generate(
                prompt="Hi, how are you?",
                max_tokens=10
            )
            
            print("âœ“ ç”ŸæˆæˆåŠŸï¼")
            print(f"  Provider: {result.provider}")
            print(f"  Model: {result.model}")
            print(f"  å“åº”: {result.text[:100]}")
            print()
            
            success = True
            break
            
        except LLMException as e:
            if e.error_type == LLMErrorType.NETWORK:
                print(f"âš ï¸  è¿žæŽ¥å¤±è´¥: {e}")
                print(f"   æç¤º: è¯·ç¡®ä¿ {name} æ­£åœ¨è¿è¡Œ")
                print(f"   å¦‚æžœä½¿ç”¨ Ollamaï¼Œè¯·è¿è¡Œ: ollama serve")
                print(f"   å¦‚æžœä½¿ç”¨ LM Studioï¼Œè¯·å¯åŠ¨ LM Studio å¹¶å¯ç”¨æœ¬åœ°æœåŠ¡å™¨")
            else:
                print(f"âŒ é”™è¯¯: {e}")
            continue
        except Exception as e:
            print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
            continue
    
    if not success:
        pytest.skip("æ‰€æœ‰æœ¬åœ°æ¨¡åž‹æœåŠ¡éƒ½ä¸å¯ç”¨")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("å…è´¹ API æµ‹è¯•ï¼ˆæ— éœ€ API Key å’Œä»˜è´¹ï¼‰")
    print("=" * 60)
    print()
    print("æœ¬æµ‹è¯•å°†éªŒè¯ä»¥ä¸‹å…è´¹é€‰é¡¹ï¼š")
    print("  1. Google Translateï¼ˆå…è´¹ç‰ˆï¼‰- å®Œå…¨å…è´¹ï¼Œæ— éœ€ API Key")
    print("  2. æœ¬åœ°æ¨¡åž‹ï¼ˆOllama/LM Studioï¼‰- æœ¬åœ°è¿è¡Œï¼Œå®Œå…¨å…è´¹")
    print()
    
    results = []
    
    # æµ‹è¯• Google Translate
    try:
        result1 = test_google_translate()
        results.append(("Google Translate", result1))
    except Exception as e:
        print(f"\nâŒ Google Translate æµ‹è¯•å¼‚å¸¸: {e}")
        results.append(("Google Translate", False))
    
    print("\n" + "=" * 60)
    
    # æµ‹è¯•æœ¬åœ°æ¨¡åž‹
    try:
        result2 = test_local_model()
        results.append(("æœ¬åœ°æ¨¡åž‹", result2))
    except Exception as e:
        print(f"\nâŒ æœ¬åœ°æ¨¡åž‹æµ‹è¯•å¼‚å¸¸: {e}")
        results.append(("æœ¬åœ°æ¨¡åž‹", False))
    
    # æ±‡æ€»ç»“æžœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æžœæ±‡æ€»")
    print("=" * 60)
    
    for name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    print(f"\næ€»è®¡: {passed}/{total} é€šè¿‡")
    
    if passed > 0:
        print("\nðŸŽ‰ è‡³å°‘æœ‰ä¸€ä¸ªå…è´¹é€‰é¡¹å¯ç”¨ï¼")
        print("   ä½ å¯ä»¥ä½¿ç”¨è¿™äº›å…è´¹é€‰é¡¹è¿›è¡Œæµ‹è¯•ï¼Œæ— éœ€é…ç½® API Key æˆ–ä»˜è´¹ã€‚")
    
    return 0 if passed > 0 else 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)


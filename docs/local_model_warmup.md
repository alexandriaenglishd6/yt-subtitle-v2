# 本地模型预热功能说明

## 概述

R3-3 任务实现了针对本地大模型的预热功能，用于避免初次调用时的冷启动延迟，并防止并发过高导致机器卡死。

## 功能特性

### 1. 自动检测本地模型

系统会自动检测配置中的 `base_url`，判断是否为本地模型：

- **本地模型标识**：
  - `localhost`
  - `127.0.0.1`
  - `0.0.0.0`
  - `::1` (IPv6 localhost)

- **示例配置**：
  ```json
  {
    "provider": "openai",
    "model": "qwen2.5:7b",
    "base_url": "http://localhost:11434/v1",
    "api_keys": {
      "openai": "ollama"
    }
  }
  ```

### 2. 后台预热

当检测到本地模型时，系统会在后台线程中执行预热：

- **预热时机**：客户端初始化时自动触发
- **执行方式**：后台线程，不阻塞初始化
- **预热请求**：
  - 极轻量的提示（"Hi"）
  - 最小输出 token（5）
  - 最低温度（0.0）
  - 较短的超时时间（最多 30 秒）

### 3. 并发控制

预热请求也使用 Semaphore 进行并发控制：

- 预热请求会占用一个 Semaphore 槽位
- 确保预热不会导致并发过高
- 与正常请求共享并发限制

### 4. 错误处理

预热失败不会影响客户端使用：

- 连接错误：可能是服务未启动，记录为调试日志
- API 错误：可能是配置问题，记录为调试日志
- 其他错误：记录为警告，但不影响功能

## 使用示例

### 配置本地模型

在 `config.json` 或 `ai_profiles.json` 中配置：

```json
{
  "translation_ai": {
    "provider": "openai",
    "model": "qwen2.5:7b",
    "base_url": "http://localhost:11434/v1",
    "timeout_seconds": 60,
    "max_retries": 2,
    "max_concurrency": 3,
    "api_keys": {
      "openai": "ollama"
    },
    "enabled": true
  }
}
```

### 日志输出

当使用本地模型时，日志中会显示：

```
[INFO] 检测到本地模型 (http://localhost:11434/v1)，开始预热...
[INFO] 本地模型预热完成: qwen2.5:7b
```

如果预热失败（例如服务未启动）：

```
[WARN] 本地模型预热失败（不影响使用）: Connection refused
```

## 技术细节

### 预热流程

1. **初始化阶段**：
   - 检测 `base_url` 是否为本地地址
   - 如果是，启动后台预热线程

2. **预热阶段**（后台线程）：
   - 创建 OpenAI 客户端
   - 使用 Semaphore 获取并发槽位
   - 发送轻量级请求
   - 记录结果或错误

3. **正常使用**：
   - 预热完成后，后续请求应该更快
   - 即使预热失败，客户端仍可正常使用

### 并发控制

```python
# 预热请求也使用 Semaphore
with self._sem:
    response = client.chat.completions.create(...)
```

这确保了：
- 预热请求不会超过并发限制
- 预热和正常请求共享并发资源
- 避免并发过高导致机器卡死

## 适用场景

### 支持的本地服务

- **Ollama**: `http://localhost:11434/v1`
- **vLLM**: `http://localhost:8000/v1`
- **其他 OpenAI 兼容服务**: 任何本地部署的服务

### 不适用的情况

- **云端 API**：不会触发预热（如 OpenAI、DeepSeek 等）
- **已热启动的服务**：如果服务已经运行，预热仍然会执行（无害）

## 性能影响

### 预热的好处

1. **减少首次调用延迟**：
   - 本地模型首次调用可能需要加载模型到内存
   - 预热可以提前完成这个过程

2. **避免冷启动**：
   - 某些本地服务在首次调用时响应较慢
   - 预热可以"唤醒"服务

### 预热的开销

1. **资源占用**：
   - 预热请求会占用一个并发槽位
   - 但只持续几秒钟

2. **网络请求**：
   - 一个极轻量的 HTTP 请求
   - 对服务负载影响很小

## 测试

运行测试脚本验证功能：

```bash
python tests/test_local_model_warmup.py
```

测试包括：
1. 本地模型检测功能
2. 预热功能（需要本地服务运行）
3. 云端模型（不应该触发预热）
4. 并发控制验证

## 故障排查

### 预热失败

如果看到预热失败的警告：

1. **检查服务是否运行**：
   ```bash
   # Ollama
   ollama list
   
   # vLLM
   curl http://localhost:8000/health
   ```

2. **检查配置**：
   - `base_url` 是否正确
   - 端口是否匹配
   - API Key 是否正确（某些服务不需要）

3. **查看日志**：
   - 预热失败不会影响功能
   - 可以查看详细日志了解原因

### 预热太慢

如果预热耗时过长：

1. **检查超时设置**：
   - 预热使用较短的超时（最多 30 秒）
   - 如果服务启动很慢，可能需要增加超时

2. **检查服务状态**：
   - 确保本地服务正常运行
   - 检查系统资源（CPU、内存）

## 相关文档

- `docs/AI_PROVIDER_EXTENSION.md`: AI 提供商扩展规范
- `docs/ai_profile_design.md`: AI Profile 配置设计
- `core/ai_providers.py`: AI 提供商实现


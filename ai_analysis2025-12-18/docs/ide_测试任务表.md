# YouTube 字幕工具 v2 - 测试任务表

> **版本**: v2.1  
> **最后更新**: 2025-12-12  
> **测试范围**: R0/R1/R2 任务 + P0/P1 功能点  
> **参考文档**: `docs/测试执行文档.md`（详细测试用例说明）

---

## 1. 文档目的

这份测试任务表用于系统化地跟踪测试进度：

- 列出所有需要测试的功能点
- 为每个测试项提供详细的测试步骤
- 明确预期结果和验证点
- 支持测试完成后更新状态

**测试优先级约定：**
- **T0 - 红线级（必测）**：影响核心功能稳定性，必须优先执行
- **T1 - 高优先级（强烈建议）**：影响使用体验，推荐执行
- **T2 - 中期优化（可选/按需）**：体验优化相关，按需执行

---

## 2. 测试前准备

### 2.1 环境检查清单

- [ ] Python 环境已配置（Python 3.8+）
- [ ] yt-dlp 已安装并可用
- [ ] 项目依赖已安装（`pip install -r requirements.txt`）
- [ ] AI API Key 已配置（如需要测试翻译/摘要功能）
- [ ] 输出目录权限正常
- [ ] 测试数据已准备（小频道、中等频道、大频道）

### 2.2 测试数据准备

**小频道 / URL 列表**（5-10 个视频）：
- 用于快速冒烟测试
- 验证基本功能是否可用

**中等频道**（50-100 个视频）：
- 用于端到端回归测试
- 验证完整流程和并发处理

**大频道**（100-500 个视频）：
- 用于压力测试和取消功能测试
- 验证长时间运行稳定性

---

## 3. T0 红线级测试（必测）

> **说明**：这些测试对应 R0 任务，是核心稳定性保障，必须优先执行。

### 3.1 T0 测试总表

| 编号 | 测试名称 | 对应任务 | 测试步骤 | 预期结果 | 状态 |
|------|----------|----------|----------|----------|------|
| **T0-1** | **AI 架构 + LLM 参数测试** | R0-1 | 见下方详细步骤 | 见下方验证点 | ⏸️ 暂缓（等更多 AI 供应商接入后测试） |
| **T0-2** | **Archive 位置 + 迁移测试** | R0-2 | 见下方详细步骤 | 见下方验证点 | ✅ 已完成 |
| **T0-3** | **Dry Run 只读测试** | R0-3 | 见下方详细步骤 | 见下方验证点 | ✅ 已完成 |
| **T0-4** | **文件写入锁 + 原子写测试** | R0-4 | 见下方详细步骤 | 见下方验证点 | ✅ 已完成 |
| **T0-5** | **临时目录清理测试** | R0-5 | 见下方详细步骤 | 见下方验证点 | ✅ 已完成 |

---

### T0-1：AI 架构 + LLM 参数测试

**对应任务**: R0-1  
**优先级**: T0（必测）  
**预计时间**: 15-20 分钟

#### 测试步骤

1. **准备测试数据**
   - 准备一个包含 5-10 个视频的小频道或 URL 列表
   - 确保测试视频有字幕（人工或自动）

2. **测试单模型配置**
   ```bash
   # CLI 方式
   python cli.py channel --url "<小频道URL>" --provider openai --model gpt-4o-mini
   ```
   - 或通过 GUI：配置同一个模型用于翻译和摘要，执行完整处理

3. **（可选增强）测试双模型配置**
   - 配置翻译模型 A（如 OpenAI）
   - 配置摘要模型 B（如 Gemini）
   - 执行完整处理，验证双模型配置

#### 验证点

- [ ] 不出现 `NameError: translation_llm / summary_llm` 等参数错误
- [ ] 所有视频均能完成"检测 → 下载 → 翻译 → 摘要"流程
- [ ] 日志中清晰打印所使用的 Provider/Model
- [ ] 翻译和摘要功能正常工作
- [ ] 输出文件正确生成

#### 预期结果

- ✅ 流水线正常运行，无参数错误
- ✅ 翻译和摘要功能正常
- ✅ 日志显示正确的 Provider/Model 信息

---

### T0-2：Archive 位置 + 迁移测试

**对应任务**: R0-2  
**优先级**: T0（必测）  
**预计时间**: 15-20 分钟

#### 测试步骤

1. **准备旧版本数据**（如有）
   - 如果之前使用过旧版本，确保 `out/archive.txt` 存在
   - 或手动创建一个测试用的 `out/archive.txt`

2. **第一次运行新版本**
   ```bash
   python cli.py channel --url "<测试频道URL>"
   ```
   - 记录运行前的文件状态

3. **检查 Archive 迁移**
   - 检查用户数据目录下的 `archives/` 目录是否存在
   - 检查是否生成了 `archives/<channel_id>.txt` 文件
   - 检查旧的 `out/archive.txt` 是否被迁移或备份

4. **第二次运行同一频道**
   ```bash
   python cli.py channel --url "<测试频道URL>"
   ```
   - 观察增量行为

#### 验证点

- [x] 第一次运行后，新 `archives/` 目录下为该频道生成独立的 archive 文件 ✅
- [x] 旧的 `out/archive.txt` 被迁移或重命名为 `.bak`，不会继续被读写 ✅
- [x] 第二次运行时，只处理新增/未处理视频，不会重新全量处理 ✅
- [x] Archive 文件格式符合 yt-dlp 规范 ✅

#### 预期结果

- ✅ Archive 文件位置正确（`archives/<channel_id>.txt`）
- ✅ 旧数据成功迁移，无数据丢失
- ✅ 增量处理正常工作

---

### T0-3：Dry Run 只读测试

**对应任务**: R0-3  
**优先级**: T0（必测）  
**预计时间**: 10-15 分钟

#### 测试步骤

1. **选择测试频道**
   - 选择一个包含 10+ 视频的频道

2. **记录运行前状态**
   - 记录以下目录的"文件列表 + 修改时间"：
     - 输出目录（`out/`，包含 `with_subtitle.txt` / `without_subtitle.txt` / `failed_urls.txt` / `failed_records.json`）
     - `archives/` 目录
     - 字幕 / 摘要输出目录（`out/original/` / `out/translated/` / `out/summary/`）

3. **执行 Dry Run**
   ```bash
   # CLI 方式
   python cli.py channel --url "<测试频道URL>" --dry-run
   ```
   - 或通过 GUI：点击"检查新视频(Dry Run)"按钮

4. **记录运行后状态**
   - 再次检查上述目录的"文件列表 + 修改时间"

#### 验证点

- [x] Dry Run 期间日志中可以看到"检测/队列执行"等行为 ✅
- [x] 运行前后对比：`archives/` 目录无新文件且无文件修改时间变化 ✅
- [x] 不生成任何输出文件（字幕、摘要、metadata 等） ✅
- [x] 不更新 Archive 文件 ✅
- [x] **会保存** `with_subtitle.txt` / `without_subtitle.txt`（检测结果记录，带 `[Dry Run]` 标记）✅
- [x] 不写入 `failed_urls.txt` / `failed_records.json`（无失败记录）✅
- [x] Dry Run 结束状态不会影响后续正常运行（已验证：Archive 未修改，不影响增量处理）✅

#### 预期结果

- ✅ Dry Run 模式下：
  - ✅ 会保存检测结果：`with_subtitle.txt` / `without_subtitle.txt`（带 `[Dry Run]` 标记）
  - ✅ 不会下载字幕、翻译、摘要
  - ✅ 不会生成输出文件（字幕、摘要、metadata 等）
  - ✅ 不会更新 Archive 文件
  - ✅ 不会写入失败记录文件
- ✅ 日志显示检测结果
- ✅ 不影响后续正常处理

---

### T0-4：文件写入锁 + 原子写测试

**对应任务**: R0-4  
**优先级**: T0（必测）  
**预计时间**: 30-40 分钟

#### 测试步骤

1. **配置高并发**
   - 在配置中将"任务并发（下载）"调为 10
   - 确保 AI 并发（`max_concurrency`）设置为合理值（如 5）

2. **执行完整流程**
   - 使用 30-50 个视频的频道运行一次完整流程
   ```bash
   python cli.py channel --url "<中等频道URL>"
   ```

3. **检查文件完整性**
   - 检查 `with_subtitle.txt`：内容正确追加，无错乱
   - 检查 `without_subtitle.txt`：内容正确追加，无错乱
   - 检查 `failed_urls.txt`：内容正确追加，无错乱
   - 检查 `failed_records.json`：JSON 格式正确，每行一个对象
   - 检查某些视频对应的字幕文件、摘要文件、`metadata.json`：内容完整，无截断

#### 验证点

- [x] `with_subtitle.txt` 内容按照逻辑正确追加，无明显错乱（如同一行被拆开、多线程交织等）✅
- [x] `without_subtitle.txt` 内容正确追加，无错乱 ✅
- [x] `failed_urls.txt` 内容正确追加，无重复 ✅
- [x] `failed_records.json` JSON 格式正确，每行一个对象，无格式错误 ✅
- [x] 不存在明显"乱码"或部分内容被截断的字幕/摘要文件 ✅
- [x] `metadata.json` 文件内容完整，JSON 格式正确 ✅
- [x] 日志中不出现文件写入相关的异常（`PermissionError` / `IOError` 等）✅

#### 预期结果

- ✅ 所有追加写入的文件内容正确，无错乱
- ✅ 所有覆盖写入的文件内容完整，无截断
- ✅ 无文件写入相关异常

---

### T0-5：临时目录清理测试

**对应任务**: R0-5  
**优先级**: T0（必测）  
**预计时间**: 15-20 分钟

#### 测试步骤

1. **正常任务测试**
   - 正常运行一个小频道任务（5-10 个视频）
   - 任务结束后检查 `temp/` 目录（通常在用户数据目录或输出目录下）

2. **失败场景测试**
   - 人为制造一个失败场景（例如让网络断开 / 播放列表中包含非法 URL）
   - 再次运行，观察失败后的 `temp/` 目录状态

3. **（可选）keep_temp_on_error 配置测试**
   - 如果配置了 `keep_temp_on_error`，测试两种情况：
     - `False`：失败后也会清理 temp
     - `True`：仅在失败时保留 temp 目录，并打印日志提示

#### 验证点

- [x] 正常完成的任务不会残留大量临时文件 ✅
- [x] 在 `keep_temp_on_error=False` 时，失败任务结束后 `temp` 目录也被清理 ✅（当前实现：无论成功/失败都清理）
- [ ] 在 `keep_temp_on_error=True` 时，失败任务结束后 `temp` 目录被保留，并有清晰日志说明保留原因（待实现）
- [x] 临时文件不会无限堆积 ✅

#### 预期结果

- ✅ 正常任务后 `temp/` 目录被清理
- ✅ 失败任务后根据配置决定是否保留 `temp/` 目录
- ✅ 无临时文件堆积问题

---

## 4. T1 高优先级测试（强烈建议）

> **说明**：这些测试对应 R1 任务，影响使用体验和长期稳定性，强烈建议执行。

### 4.1 T1 测试总表

| 编号 | 测试名称 | 对应任务 | 测试步骤 | 预期结果 | 状态 |
|------|----------|----------|----------|----------|------|
| **T1-1** | **错误分类与失败记录测试** | R1-1 | 见下方详细步骤 | 见下方验证点 | 🔄 部分完成 |
| **T1-2** | **任务取消功能测试** | R1-2 | 见下方详细步骤 | 见下方验证点 | 🔄 部分完成 |
| **T1-3** | **代理健康管理测试** | R1-3 | 见下方详细步骤 | 见下方验证点 | ✅ 已完成 |
| **T1-4** | **AI 并发限流测试** | R1-4 | 见下方详细步骤 | 见下方验证点 | ✅ 已完成 |

---

### T1-1：错误分类与失败记录测试

**对应任务**: R1-1  
**优先级**: T1（强烈建议）  
**预计时间**: 15-20 分钟  
**状态**: ✅ 部分完成（RATE_LIMIT 暂缓）

#### 测试步骤

1. **测试网络错误（NETWORK）** ✅
   - 使用无效代理（如 `http://127.0.0.1:9999`）
   - 执行处理任务，观察错误分类
   - ✅ 已验证：错误被正确分类为 `network`

2. **测试限流错误（RATE_LIMIT）** ⏸️
   - 使用高并发触发 AI 限流（如配置多个任务同时调用 AI）
   - 观察错误分类
   - ⏸️ **暂缓**：当前只有 OpenAI 和 Claude，型号较少且价格昂贵，等增加更多 AI 供应商后再测试

3. **测试认证错误（AUTH）** ✅
   - 使用无效的 AI API Key
   - 执行处理任务，观察错误分类
   - ✅ 已验证：错误被正确分类为 `auth`，错误信息包含详细的 API Key 错误说明

4. **测试内容错误（CONTENT）** ✅
   - 使用 `OFFICIAL_ONLY` 策略但无可用官方字幕的视频
   - 观察错误分类
   - ✅ 已验证：错误被正确分类为 `content`

5. **检查失败记录**
   - 检查 `failed_detail.log`：格式正确，包含错误类型
   - 检查 `failed_urls.txt`：URL 列表正确
   - 检查 `failed_records.json`：JSON 记录包含 `error_type` 字段

#### 验证点

- [x] 网络错误被正确分类为 `NETWORK` ✅
- [ ] 限流错误被正确分类为 `RATE_LIMIT` ⏸️ 暂缓（等更多 AI 供应商）
- [x] 认证错误被正确分类为 `AUTH` ✅
- [x] 内容错误被正确分类为 `CONTENT` ✅
- [x] `failed_detail.log` 格式符合规范，包含 `error=<error_type>` ✅
- [x] `failed_records.json` 中每条记录包含 `error_type` 字段 ✅
- [x] 错误信息清晰易懂 ✅

#### 预期结果

- ✅ 错误被正确分类，不是全部 `UNKNOWN`
- ✅ 失败记录文件格式正确，包含错误类型信息

#### 测试完成情况

**已完成测试项**：
- ✅ **NETWORK 错误**：已验证（使用无效代理，错误被正确分类为 `network`）
- ✅ **AUTH 错误**：已验证（使用无效 API Key，错误被正确分类为 `auth`，错误信息包含详细的 API Key 错误说明）
- ✅ **CONTENT 错误**：已验证（使用 `OFFICIAL_ONLY` 策略但无可用官方字幕，错误被正确分类为 `content`）
- ✅ **失败记录格式**：已验证（`failed_detail.log` 和 `failed_records.json` 格式正确，包含 `error_type` 字段）

**暂缓测试项**：
- ⏸️ **RATE_LIMIT 错误**：暂缓（当前只有 OpenAI 和 Claude，型号较少且价格昂贵，等增加更多 AI 供应商后再测试）

**备注**：
- 已完成测试项均通过验证，错误分类逻辑正确
- RATE_LIMIT 错误测试计划在增加更多 AI 供应商（如 Gemini、DeepSeek 等）后补充测试

---

### T1-2：任务取消功能测试

**对应任务**: R1-2  
**优先级**: T1（强烈建议）  
**预计时间**: 20-30 分钟  
**状态**: 🔄 部分完成（摘要阶段暂缓）

#### 测试步骤

1. **准备大频道**
   - 选择一个包含 100+ 视频的大频道

2. **测试下载阶段取消** ✅
   - 开始处理任务
   - 在下载阶段中途点击"停止任务"按钮（GUI）或按 Ctrl+C（CLI）
   - 观察取消响应时间
   - ✅ 已验证：取消功能正常，响应及时

3. **测试翻译阶段取消** ✅
   - 重新开始处理任务
   - 在翻译阶段中途取消
   - 观察取消响应时间
   - ✅ 已验证：取消功能正常，响应及时（在当前字幕块翻译完成后停止）

4. **测试摘要阶段取消** ⏸️
   - 重新开始处理任务
   - 在摘要阶段中途取消
   - 观察取消响应时间
   - ⏸️ 暂缓：等增加更多 AI 供应商后再测试

5. **检查取消后状态** ✅
   - 检查是否有异常大量的临时文件残留
   - 检查是否有半截输出文件
   - 检查日志中是否有"任务被用户取消"的记录
   - ✅ 已验证：无异常文件残留，日志记录清晰

#### 验证点

- ✅ 每次取消都在合理时间内响应（< 5 秒），任务不会无限挂起
- ✅ 取消后不会留下异常大量的临时文件或半截输出
- ✅ 日志中有明确"任务被用户取消"的记录，并带有对应的上下文信息（任务 ID 或频道 URL）
- ✅ 取消后可以正常开始新的任务

#### 测试结果

**已完成测试项**：
- ✅ **下载阶段取消**：已验证（取消功能正常，响应及时）
- ✅ **翻译阶段取消**：已验证（取消功能正常，在当前字幕块翻译完成后停止）
- ✅ **取消后状态检查**：已验证（无异常文件残留，日志记录清晰）

**暂缓测试项**：
- ⏸️ **摘要阶段取消**：暂缓（等增加更多 AI 供应商后再测试）

**备注**：
- 下载和翻译阶段的取消功能已正常工作
- 翻译阶段的取消响应：由于 Google Translate 的 `translate()` 方法是阻塞调用，取消响应会在当前字幕块翻译完成后生效，这是第三方库的限制
- 摘要阶段取消测试计划在增加更多 AI 供应商（如 Gemini、DeepSeek 等）后补充测试

---

### T1-3：代理健康管理测试

**对应任务**: R1-3  
**优先级**: T1（强烈建议）  
**预计时间**: 20-30 分钟

#### 测试步骤

**方法一：使用测试脚本（推荐，高效）**
1. 运行测试脚本：`python test_proxy_health.py`
   - 脚本使用降低的阈值（2次失败）和缩短的延迟（10秒）
   - 可以快速验证所有功能点
   - 无需实际运行任务，几分钟内完成测试

**方法二：实际任务测试**
1. **配置多个代理**
   - 在配置中添加多个代理（包括一些无效代理）

2. **执行处理任务**
   - 使用中等频道（50-100 个视频）执行处理
   - 观察代理使用情况

3. **观察代理健康管理**
   - 观察无效代理是否被标记为 unhealthy
   - 观察系统是否自动切换到健康代理
   - 观察 unhealthy 代理是否在 10 分钟后被重新探测

4. **测试全部代理失效场景**
   - 配置所有代理为无效
   - 观察系统是否尝试直连或使用失败最少的代理

#### 验证点

- [x] 同一代理连续失败 ≥5 次时被标记为 unhealthy（测试脚本验证：阈值2次即触发）
- [x] unhealthy 代理在后续 10 分钟内避免继续使用（测试脚本验证：延迟10秒）
- [x] 定时对 unhealthy 代理做轻量探测，请求成功则恢复为 healthy（测试脚本验证：恢复机制正常）
- [x] 所有代理都 unhealthy 时，可尝试直连或失败最少的代理（测试脚本验证：返回 None 表示使用直连）
- [x] 坏代理不会卡死整个任务（实际任务测试验证：已确认）

#### 预期结果

- ✅ 代理健康管理正常工作
- ✅ 自动切换和恢复机制有效
- ✅ 不会因坏代理导致任务卡死

#### 测试技巧

- **快速测试**：使用 `test_proxy_health.py` 脚本，使用降低的阈值和延迟参数
- **实际验证**：在真实任务中观察代理切换和恢复行为
- **日志观察**：检查日志中是否有 "代理标记为不健康"、"代理已恢复健康" 等消息

---

### T1-4：AI 并发限流测试

**对应任务**: R1-4  
**优先级**: T1（强烈建议）  
**预计时间**: 30-40 分钟  
**状态**: ✅ 已完成

#### 测试步骤

**方法一：使用测试脚本（推荐，快速验证）**
1. 运行测试脚本：`python test_ai_concurrency.py`
   - 脚本会发起多个并发 AI 请求（默认为配置值的3倍）
   - 监控实际的并发峰值
   - 验证并发限制是否生效
   - 几分钟内完成测试

**方法二：实际任务测试（完整验证）**
1. **配置并发参数**
   - 下载并发：10（在 config.json 中设置 `concurrency`）
   - AI 并发（`max_concurrency`）：2 或 3（在 config.json 中的 `translation_ai.max_concurrency` 设置）

2. **执行处理任务**
   - 使用一个包含 50+ 视频的频道，执行完整任务
   ```bash
   python cli.py channel --url "<中等频道URL>" --run
   ```

3. **观察 AI 调用情况**
   - 观察日志中 AI 调用的并发情况
   - 观察是否出现大规模限流 / 429 / 超时错误
   - 观察 AI Provider 的 QPS / 调用情况（可通过日志间接观察）

#### 验证点

- [x] 日志中可以看到下载任务并发较高（接近 10）✅
- [x] AI 调用的"并发峰值"受 Semaphore 限制（通过测试脚本验证）✅
- [x] 没有出现因为瞬时高并发导致的大面积 429 / 超时（测试脚本：0% 失败率）✅
- [x] 偶发错误会被正常重试或记录为失败 URL，不会导致整个任务崩溃（测试脚本：所有请求都能完成）✅
- [x] AI 调用稳定，不会因并发过高导致崩溃（测试脚本：15/15 请求成功）✅

**测试说明**：
- 使用 `test_ai_concurrency.py` 脚本进行快速验证
- 测试结果显示：所有请求成功完成（15/15），0% 失败率
- 注意：监控方法无法直接测量 Semaphore 保护内的实际并发数（会包括等待的线程）
- 如果实际任务中没有大量并发错误，说明 Semaphore 正常工作

#### 预期结果

- ✅ AI 并发受 `Semaphore` 限制，不超过配置值
- ✅ 无大规模限流错误
- ✅ 任务稳定运行

#### 测试技巧

- **快速验证**：使用 `test_ai_concurrency.py` 脚本，几分钟内完成并发限制验证
- **实际验证**：在实际任务中观察日志，检查是否有 429 错误和任务稳定性
- **监控方法**：
  - 测试脚本：使用事件时间线分析，估算实际并发峰值
  - 实际任务：观察日志中的 AI 调用时间分布，如果调用时间分布合理（不是同时大量发起），说明限流有效
  - 检查日志中是否有 "请求频率限制" 或 "429" 错误

---

## 5. T2 中期优化测试（可选/按需）

> **说明**：这些测试对应 R2 任务，主要是体验优化，按需执行。

### 5.1 T2 测试总表

| 编号 | 测试名称 | 对应任务 | 测试步骤 | 预期结果 | 状态 |
|------|----------|----------|----------|----------|------|
| **T2-1** | **结构化失败记录测试** | R2-1 | 见下方详细步骤 | 见下方验证点 | ✅ 已完成 |

---

### T2-1：结构化失败记录测试

**对应任务**: R2-1  
**优先级**: T2（可选/按需）  
**预计时间**: 10-15 分钟

#### 测试步骤

1. **运行包含失败场景的任务**
   - 使用无效 URL 或网络错误触发失败
   - 执行处理任务

2. **检查 `failed_records.json` 文件**
   - 检查文件是否存在（`out/failed_records.json`）
   - 打开文件，检查 JSON 格式

3. **验证 JSON 记录格式**
   - 验证每行一个 JSON 对象（JSONL 格式）
   - 验证必需字段：`video_id`, `url`, `stage`, `error_type`, `timestamp`
   - 验证可选字段：`run_id`, `reason`, `channel_id`, `channel_name`

4. **验证一致性**
   - 对比 `failed_records.json` 与 `failed_detail.log` 的记录
   - 对比 `failed_records.json` 与 `failed_urls.txt` 的记录

#### 验证点

- [x] `failed_records.json` 文件正确生成 ✅
- [x] JSON 记录格式正确（JSONL 格式，每行一个对象）✅
- [x] 必需字段完整：`video_id`, `url`, `stage`, `error_type`, `timestamp` ✅
- [x] 可选字段正确填充（如有）：`run_id`, `reason`, `channel_id`, `channel_name` ✅
- [x] 记录与 `failed_detail.log` 和 `failed_urls.txt` 一致 ✅
- [x] JSON 格式正确，可以正常解析 ✅

**测试说明**：
- 使用 `test_failed_records.py` 脚本进行验证
- 测试结果显示：所有验证点通过
- JSON 记录格式正确（JSONL 格式，每行一个对象）
- 必需字段完整，可选字段正确填充
- 记录与其他失败记录文件（failed_detail.log, failed_urls.txt）一致

#### 预期结果

- ✅ `failed_records.json` 文件正确生成
- ✅ JSON 记录格式正确，字段完整
- ✅ 记录与其他失败记录文件一致

---

## 6. P0/P1 功能点测试

> **说明**：这些测试对应 P0/P1 功能任务，验证核心功能是否正常工作。

### 6.1 P0 功能点测试总表

| 编号 | 功能点 | 优先级 | 测试用例 | 详细步骤 | 状态 |
|------|--------|--------|----------|----------|------|
| **P0-T1** | 配置管理 | 必测 | TC-CLI-001, TC-GUI-008 | 见 `docs/测试执行文档.md` 第 3、4 章 | ✅ 已完成（GUI 运行参数配置测试通过，TC-GUI-008） |
| **P0-T2** | CLI 入口 | 必测 | TC-CLI-001 ~ TC-CLI-006 | 见 `docs/测试执行文档.md` 第 3 章 | ✅ 已完成（所有测试用例通过） |
| **P0-T3** | 视频解析 | 必测 | TC-CLI-001, TC-CLI-004 | 见 `docs/测试执行文档.md` 第 3 章 | ✅ 已完成 |
| **P0-T4** | 增量管理 | 必测 | TC-CLI-003, TC-GUI-011, TC-GUI-012 | 见 `docs/测试执行文档.md` 第 3、4 章 | ✅ 已完成（CLI 部分） |
| **P0-T5** | 字幕检测 | 必测 | TC-CLI-001, TC-GUI-001 | 见 `docs/测试执行文档.md` 第 3、4 章 | ✅ 已完成（CLI 部分） |
| **P0-T6** | Dry Run | 必测 | TC-CLI-001, TC-CLI-004, TC-GUI-001 | 见 `docs/测试执行文档.md` 第 3、4 章 | ✅ 已完成（CLI 部分，已在 T0-3 验证） |
| **P0-T7** | 字幕下载 | 必测 | TC-CLI-002, TC-GUI-002 | 见 `docs/测试执行文档.md` 第 3、4 章 | ✅ 已完成（CLI 部分） |
| **P0-T8** | AI 翻译 | 必测 | TC-CLI-002, TC-GUI-002 | 见 `docs/测试执行文档.md` 第 3、4 章 | ✅ 已完成（已在昨天多次测试验证） |
| **P0-T9** | 输出模块 | 必测 | TC-CLI-002, TC-GUI-002 | 见 `docs/测试执行文档.md` 第 3、4 章 | ✅ 已完成（CLI 部分） |
| **P0-T10** | 失败记录 | 必测 | TC-CLI-007, TC-CLI-008 | 见 `docs/测试执行文档.md` 第 3 章 | ✅ 已完成（CLI 部分，已在 T2-1 部分验证） |
| **P0-T11** | 并发执行 | 必测 | TC-CLI-002, TC-GUI-002 | 见 `docs/测试执行文档.md` 第 3、4 章 | ✅ 已完成（CLI 部分，已在 T1-4 和测试脚本验证） |
| **P0-T12** | GUI 骨架 | 必测 | TC-GUI-001 ~ TC-GUI-016 | 见 `docs/测试执行文档.md` 第 4 章 | ✅ 已完成 |

> **注意**：P0 功能点的详细测试步骤请参考 `docs/测试执行文档.md` 第 3、4 章的测试用例说明。

### 6.2 P1 功能点测试总表

| 编号 | 功能点 | 优先级 | 测试用例 | 详细步骤 | 状态 |
|------|--------|--------|----------|----------|------|
| **P1-T1** | URL 列表模式 | 必测 | TC-CLI-004, TC-CLI-005, TC-GUI-004 ~ TC-GUI-007 | 见 `docs/测试执行文档.md` 第 3、4 章 | ✅ 已完成 |
| **P1-T2** | 双语字幕 | 建议 | TC-CLI-002, TC-GUI-002 | 见 `docs/测试执行文档.md` 第 3、4 章 | ⬜ 未开始 |
| **P1-T3** | 增量高级选项 | 建议 | TC-CLI-003, TC-GUI-003, TC-GUI-011, TC-GUI-012 | 见 `docs/测试执行文档.md` 第 3、4 章 | ⬜ 未开始 |
| **P1-T4** | 错误分类 | 建议 | TC-CLI-007, TC-CLI-008, TC-GUI-002 | 见 `docs/测试执行文档.md` 第 3、4 章 | ⬜ 未开始 |
| **P1-T5** | 进度展示 | 建议 | TC-CLI-002, TC-CLI-005, TC-GUI-002, TC-GUI-007 | 见 `docs/测试执行文档.md` 第 3、4 章 | ⬜ 未开始 |
| **P1-T6** | 日志视图 | 建议 | TC-GUI-013, TC-GUI-014 | 见 `docs/测试执行文档.md` 第 4 章 | ⬜ 未开始 |

> **注意**：P1 功能点的详细测试步骤请参考 `docs/测试执行文档.md` 第 3、4 章的测试用例说明。

---

## 7. 测试执行建议

### 7.1 推荐执行顺序

**阶段一：快速冒烟测试（30-45 分钟）**
1. GUI 启动测试（5 分钟）
2. T0-3 Dry Run 测试（10 分钟）
3. T0-1 AI 架构测试（15 分钟）
4. T2-1 失败记录测试（10 分钟）

**阶段二：T0 红线级完整测试（1-2 小时）**
1. T0-1：AI 架构 + LLM 参数
2. T0-2：Archive 位置 + 迁移
3. T0-3：Dry Run 只读
4. T0-4：文件写入锁 + 原子写
5. T0-5：临时目录清理

**阶段三：T1 高优先级测试（1-2 小时）**
1. T1-1：错误分类与失败记录
2. T1-2：任务取消功能
3. T1-3：代理健康管理
4. T1-4：AI 并发限流

**阶段四：P0/P1 功能点测试（2-3 小时）**
- 按照测试执行文档第 3、4 章的测试用例逐一执行

### 7.2 测试记录

建议为每个测试阶段创建测试记录文件（保存在 `docs/test_reports/`）：

- `v2.1_t0_smoke_test.md` - T0 冒烟测试记录
- `v2.1_t0_full_test.md` - T0 完整测试记录
- `v2.1_t1_test.md` - T1 测试记录
- `v2.1_p0_test.md` - P0 功能点测试记录

每个测试记录应包含：
- 测试日期和时间
- 测试人员
- 使用的测试数据（频道 URL、视频数量等）
- 测试结果（通过/失败）
- 发现的问题（如有）
- 日志片段（如有问题）

---

## 8. 状态更新说明

测试完成后，请更新对应测试项的状态：

- ⬜ **未开始**：尚未执行测试
- 🔄 **进行中**：正在执行测试
- ✅ **已完成**：测试通过
- ❌ **失败**：测试失败，需要修复
- ⚠️ **部分通过**：部分验证点通过，但有问题需要修复

更新方式：直接在表格中修改"状态"列的值。

---

**文档结束**

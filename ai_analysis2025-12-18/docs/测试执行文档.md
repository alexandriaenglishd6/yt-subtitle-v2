# YouTube 字幕工具 v2 - 测试执行文档

> **版本**: v2.1  
> **最后更新**: 2025-12-12  
> **测试范围**: P0 全部任务 + P1-1 至 P1-6

---

## 目录

1. [测试概述](#1-测试概述)
2. [测试环境准备](#2-测试环境准备)
3. [CLI 功能测试](#3-cli-功能测试)
4. [GUI 功能测试](#4-gui-功能测试)
5. [功能点测试清单](#5-功能点测试清单)
6. [测试用例详细说明](#6-测试用例详细说明)
7. [问题记录模板](#7-问题记录模板)
8. [测试报告模板](#8-测试报告模板)

---

## 1. 测试概述

### 1.1 测试目标

验证 YouTube 字幕工具 v2 的所有已实现功能是否正常工作，包括：
- **P0 任务**：核心功能（CLI、字幕检测、下载、翻译、摘要、输出、GUI 基础）
- **P1 任务**：增强功能（双语字幕、URL 列表模式、增量高级选项、错误分类、进度展示、日志视图）

### 1.2 测试范围

| 模块 | 功能点 | 状态 |
|------|--------|------|
| CLI 命令行接口 | 频道模式、URL 列表模式、Dry Run | ✅ 需测试 |
| GUI 图形界面 | 频道模式、URL 列表模式、配置管理 | ✅ 需测试 |
| 字幕处理 | 检测、下载、翻译、摘要 | ✅ 需测试 |
| 输出管理 | 目录结构、文件生成、双语字幕 | ✅ 需测试 |
| 增量处理 | Archive 管理、强制重跑 | ✅ 需测试 |
| 错误处理 | 错误分类、重试策略 | ✅ 需测试 |
| 进度展示 | 进度条、ETA、当前任务 | ✅ 需测试 |
| 日志系统 | 日志过滤、自动滚动 | ✅ 需测试 |

### 1.3 测试策略

- **功能测试**：验证所有功能点是否按预期工作
- **集成测试**：验证模块间协作是否正常
- **界面测试**：验证 GUI 交互和显示是否正常
- **边界测试**：验证异常情况处理是否合理

---

## 2. 测试环境准备

### 2.1 系统要求

- **操作系统**: Windows 10/11, macOS, Linux
- **Python 版本**: Python 3.8+
- **依赖工具**: yt-dlp（需在系统 PATH 中）

### 2.2 环境配置

#### 2.2.1 安装依赖

```bash
# 安装 Python 依赖
pip install -r requirements.txt

# 验证 yt-dlp 是否可用
yt-dlp --version
```

#### 2.2.2 配置准备

1. **创建测试配置**（可选）
   - 复制 `config.json` 到用户数据目录
   - 配置 AI API Key（如需要测试翻译/摘要功能）
   - 配置代理（如需要）

2. **准备测试数据**
   - 准备一个小的测试频道 URL（建议 < 10 个视频）
   - 准备一个包含 2-5 个视频 URL 的文本文件
   - 确保测试视频有字幕（人工或自动）

#### 2.2.3 测试数据示例

**测试频道 URL**（示例）:
```
https://www.youtube.com/@testchannel
```

**测试 URL 列表文件** (`test_urls.txt`):
```
https://www.youtube.com/watch?v=xxxxx
https://www.youtube.com/watch?v=yyyyy
https://www.youtube.com/watch?v=zzzzz
```

### 2.3 测试前检查清单

- [ ] Python 环境已配置
- [ ] yt-dlp 已安装并可用
- [ ] 项目依赖已安装
- [ ] 测试数据已准备
- [ ] AI API Key 已配置（如需要）
- [ ] 输出目录权限正常

---

## 3. CLI 功能测试

### 3.1 频道模式测试

#### 测试用例 TC-CLI-001: 频道 Dry Run

**步骤**:
```bash
python cli.py channel --url <测试频道URL> --dry-run
```

**预期结果**:
- [ ] 成功获取视频列表
- [ ] 显示每个视频的字幕检测结果
- [ ] 显示人工字幕和自动字幕的详细列表
- [ ] 输出统计信息（有字幕/无字幕数量）
- [ ] 不下载、不翻译、不更新增量

**验证点**:
- 日志输出格式正确
- 字幕检测结果准确
- 无错误或异常

---

#### 测试用例 TC-CLI-002: 频道完整处理

**步骤**:
```bash
python cli.py channel --url <测试频道URL> --run
```

**预期结果**:
- [ ] 成功获取视频列表
- [ ] 执行字幕检测
- [ ] 下载原始字幕
- [ ] 执行翻译（如配置）
- [ ] 生成摘要（如配置）
- [ ] 创建输出目录结构
- [ ] 更新增量记录
- [ ] 显示进度信息（当前处理视频、ETA）

**验证点**:
- 输出目录结构符合规范
- 生成的文件完整
- 增量记录正确更新
- 进度信息正确显示

---

#### 测试用例 TC-CLI-003: 频道强制重跑

**步骤**:
```bash
python cli.py channel --url <测试频道URL> --run --force
```

**预期结果**:
- [ ] 忽略增量记录
- [ ] 重新处理所有视频
- [ ] 更新增量记录

**验证点**:
- 所有视频都被处理（即使之前已处理过）
- 增量记录正确更新

---

### 3.2 URL 列表模式测试

#### 测试用例 TC-CLI-004: URL 列表 Dry Run

**步骤**:
```bash
python cli.py urls --file test_urls.txt --dry-run
```

**预期结果**:
- [ ] 成功读取 URL 列表文件
- [ ] 显示每个视频的字幕检测结果
- [ ] 显示详细字幕列表
- [ ] 输出统计信息

**验证点**:
- URL 列表解析正确
- 字幕检测结果准确
- 无错误或异常

---

#### 测试用例 TC-CLI-005: URL 列表完整处理

**步骤**:
```bash
python cli.py urls --file test_urls.txt --run
```

**预期结果**:
- [ ] 成功处理所有 URL
- [ ] 显示进度信息
- [ ] 创建输出目录结构
- [ ] 更新批次增量记录

**验证点**:
- 所有 URL 都被处理
- 输出目录结构正确
- 进度信息正确显示

---

### 3.3 Cookie 测试

#### 测试用例 TC-CLI-006: Cookie 测试

**步骤**:
```bash
python cli.py test-cookie
```

**预期结果**:
- [ ] 成功读取 Cookie 配置
- [ ] 执行测试请求
- [ ] 显示 Cookie 可用性
- [ ] 显示所在地区（如可获取）

**验证点**:
- Cookie 测试结果准确
- 错误信息清晰（如 Cookie 无效）

---

### 3.4 错误处理测试

#### 测试用例 TC-CLI-007: 网络错误处理

**测试场景**: 断开网络或使用无效代理

**预期结果**:
- [ ] 错误被正确分类（网络错误）
- [ ] 显示清晰的错误信息
- [ ] 重试机制正常工作（如配置）
- [ ] 失败记录写入 `failed_detail.log`

---

#### 测试用例 TC-CLI-008: 字幕不存在处理

**测试场景**: 使用无字幕的视频 URL

**预期结果**:
- [ ] 错误被正确分类（内容错误）
- [ ] 显示清晰的错误信息
- [ ] 失败记录正确写入

---

---

## 4. GUI 功能测试

### 4.1 频道模式测试

#### 测试用例 TC-GUI-001: 频道 Dry Run

**步骤**:
1. 启动 GUI: `python main.py`
2. 切换到"频道模式"页面
3. 输入测试频道 URL
4. 点击"检查新视频(Dry Run)"按钮

**预期结果**:
- [ ] 日志面板显示"开始 Dry Run 检测"
- [ ] 显示视频列表获取进度
- [ ] 显示每个视频的字幕检测结果
- [ ] 显示详细字幕列表（人工字幕、自动字幕）
- [ ] 统计信息面板显示检测结果
- [ ] 不下载、不翻译、不更新增量

**验证点**:
- UI 响应正常
- 日志输出正确
- 统计信息准确
- 无错误弹窗

---

#### 测试用例 TC-GUI-002: 频道完整处理

**步骤**:
1. 在频道模式页面输入 URL
2. 点击"开始处理"按钮

**预期结果**:
- [ ] 日志面板显示处理进度
- [ ] 显示当前正在处理的视频
- [ ] 显示预计剩余时间（完成一定数量后）
- [ ] 统计信息实时更新
- [ ] 处理完成后显示最终统计
- [ ] 显示错误分类统计（如有失败）

**验证点**:
- 进度信息正确显示
- ETA 计算合理
- 统计信息准确
- 错误分类正确

---

#### 测试用例 TC-GUI-003: 强制重跑功能

**步骤**:
1. 在频道模式页面输入已处理过的频道 URL
2. 勾选"强制重跑"复选框
3. 点击"开始处理"

**预期结果**:
- [ ] 忽略增量记录
- [ ] 重新处理所有视频
- [ ] 更新增量记录

**验证点**:
- 所有视频都被处理
- 增量记录正确更新

---

### 4.2 URL 列表模式测试

#### 测试用例 TC-GUI-004: URL 列表输入和去重

**步骤**:
1. 切换到"URL 列表模式"页面
2. 在文本框中输入多个 URL（包含重复）
3. 点击"去重"按钮

**预期结果**:
- [ ] 日志面板显示去重结果
- [ ] 重复 URL 被移除
- [ ] 文本框更新为去重后的 URL 列表

**验证点**:
- 去重逻辑正确（基于视频 ID）
- 日志提示正确显示

---

#### 测试用例 TC-GUI-005: URL 列表导入文件

**步骤**:
1. 在 URL 列表模式页面
2. 点击"导入文件"按钮
3. 选择包含 URL 的文本文件

**预期结果**:
- [ ] 成功读取文件
- [ ] URL 被提取并添加到文本框
- [ ] 日志显示导入的 URL 数量

**验证点**:
- 文件读取正确
- URL 提取准确
- 混合文本中的 URL 也能正确提取

---

#### 测试用例 TC-GUI-006: URL 列表清空

**步骤**:
1. 在 URL 列表模式页面输入一些 URL
2. 点击"清空"按钮

**预期结果**:
- [ ] 日志面板显示清空提示
- [ ] 文本框被清空并恢复占位符

**验证点**:
- 清空功能正常
- 日志提示正确显示

---

#### 测试用例 TC-GUI-007: URL 列表完整处理

**步骤**:
1. 在 URL 列表模式页面输入多个 URL
2. 点击"开始处理"按钮

**预期结果**:
- [ ] 显示处理进度
- [ ] 显示当前正在处理的视频
- [ ] 显示预计剩余时间
- [ ] 统计信息实时更新
- [ ] 处理完成后显示最终统计

**验证点**:
- 所有 URL 都被处理
- 进度信息正确
- 统计信息准确

---

### 4.3 配置管理测试

#### 测试用例 TC-GUI-008: 运行参数配置

**步骤**:
1. 切换到"运行参数"页面
2. 修改并发数量
3. 修改重试次数
4. 勾选/取消"强制重跑"
5. 点击"保存设置"

**预期结果**:
- [x] 配置成功保存 ✅
- [x] 日志面板显示保存成功提示 ✅
- [x] 配置在下次启动时生效 ✅

**验证点**:
- 配置保存正确
- 配置加载正确
- 日志提示正确显示

---

#### 测试用例 TC-GUI-009: 语言配置

**步骤**:
1. 切换到频道模式或 URL 列表模式
2. 修改"字幕翻译目标语言"
3. 修改"摘要语言"
4. 修改"双语字幕模式"
5. 修改"翻译策略"
6. 点击"保存语言配置"

**预期结果**:
- [ ] 配置成功保存
- [ ] 日志面板显示保存成功提示
- [ ] 配置在处理时生效

**验证点**:
- 配置保存正确
- 配置在处理时正确应用

---

#### 测试用例 TC-GUI-010: 网络和 AI 配置

**步骤**:
1. 切换到"网络 & AI"页面
2. 配置代理列表
3. 配置 Cookie
4. 配置 AI 提供商和模型
5. 点击"保存"按钮

**预期结果**:
- [ ] 配置成功保存
- [ ] Cookie 测试功能正常
- [ ] 代理配置生效

**验证点**:
- 配置保存正确
- Cookie 测试结果准确
- 代理配置正确应用

---

### 4.4 增量管理测试

#### 测试用例 TC-GUI-011: 清空频道 Archive

**步骤**:
1. 切换到"系统与工具"页面
2. 在"Archive 管理"区域输入频道 ID
3. 点击"清空该频道记录"按钮
4. 确认操作

**预期结果**:
- [ ] 显示确认对话框
- [ ] 确认后清空该频道的增量记录
- [ ] 日志面板显示操作结果

**验证点**:
- 确认对话框正常显示
- Archive 文件正确清空
- 日志提示正确显示

---

#### 测试用例 TC-GUI-012: 清空所有 Archive

**步骤**:
1. 切换到"系统与工具"页面
2. 点击"清空所有记录"按钮
3. 确认操作

**预期结果**:
- [ ] 显示确认对话框
- [ ] 确认后清空所有增量记录
- [ ] 日志面板显示操作结果

**验证点**:
- 确认对话框正常显示
- 所有 Archive 文件正确清空
- 日志提示正确显示

---

### 4.5 日志视图测试

#### 测试用例 TC-GUI-013: 日志级别过滤

**步骤**:
1. 执行一些操作产生不同级别的日志（INFO、WARN、ERROR）
2. 在日志面板的"过滤"下拉框中选择不同级别

**预期结果**:
- [ ] 选择"全部"时显示所有日志
- [ ] 选择"INFO"时只显示 INFO 级别日志
- [ ] 选择"WARN"时只显示 WARN 级别日志
- [ ] 选择"ERROR"时只显示 ERROR 级别日志
- [ ] 过滤立即生效

**验证点**:
- 过滤功能正常
- 日志显示正确
- 切换流畅

---

#### 测试用例 TC-GUI-014: 暂停自动滚动

**步骤**:
1. 执行一些操作产生日志
2. 取消选中"自动滚动"复选框
3. 继续执行操作产生新日志
4. 重新选中"自动滚动"复选框

**预期结果**:
- [ ] 取消选中后，新日志不自动滚动到底部
- [ ] 可以手动滚动查看历史日志
- [ ] 重新选中后，新日志自动滚动到底部

**验证点**:
- 自动滚动控制正常
- 手动滚动正常
- 切换流畅

---

### 4.6 UI 国际化测试

#### 测试用例 TC-GUI-015: 语言切换

**步骤**:
1. 在工具栏的语言下拉框中选择"中文"
2. 观察 UI 文本
3. 切换到"English"
4. 观察 UI 文本

**预期结果**:
- [ ] 所有 UI 文本正确翻译
- [ ] 切换流畅无闪烁
- [ ] 配置正确保存

**验证点**:
- 翻译完整准确
- 切换无错误
- 配置持久化

---

#### 测试用例 TC-GUI-016: 主题切换

**步骤**:
1. 在工具栏的主题下拉框中选择不同主题
2. 观察 UI 外观

**预期结果**:
- [ ] 主题切换正常
- [ ] 颜色方案正确应用
- [ ] 配置正确保存

**验证点**:
- 主题切换流畅
- 颜色正确
- 配置持久化

---

## 5. 功能点测试清单

### 5.1 P0 功能测试清单

> **优先级说明：**
> - **必测**：每个版本发布前必须至少跑一次，直接影响"能不能用/会不会翻车"
> - **建议**：非每次必跑，但推荐有时间就跑，主要影响体验、观感、排查效率
> - **可选**：主要影响体验或外观，按需执行

| 功能点 | 优先级 | 测试用例 | 状态 | 备注 |
|--------|--------|----------|------|------|
| 配置管理     | 必测     | TC-CLI-001, TC-GUI-008                     | ⬜ | |
| 日志系统     | 建议     | TC-GUI-013, TC-GUI-014                     | ⬜ | |
| CLI 入口    | 必测     | TC-CLI-001 ~ TC-CLI-006                    | ⬜ | |
| 视频解析     | 必测     | TC-CLI-001, TC-CLI-004                     | ⬜ | |
| 增量管理     | 必测     | TC-CLI-003, TC-GUI-011, TC-GUI-012         | ⬜ | |
| 字幕检测     | 必测     | TC-CLI-001, TC-GUI-001                     | ⬜ | |
| Dry Run    | 必测     | TC-CLI-001, TC-CLI-004, TC-GUI-001         | ⬜ | |
| 字幕下载     | 必测     | TC-CLI-002, TC-GUI-002                     | ⬜ | |
| 语言配置     | 建议     | TC-GUI-009                                  | ⬜ | |
| AI 翻译    | 必测     | TC-CLI-002, TC-GUI-002                     | ⬜ | |
| AI 摘要    | 建议     | TC-CLI-002, TC-GUI-002                     | ⬜ | |
| 输出模块     | 必测     | TC-CLI-002, TC-GUI-002                     | ⬜ | 检查目录结构与文件命名/内容是否符合规范 |
| 失败记录     | 必测     | TC-CLI-007, TC-CLI-008                     | ⬜ | 确认失败记录文件存在且信息完整，包括 failed_records.json |
| 并发执行     | 必测     | TC-CLI-002, TC-GUI-002                     | ⬜ | 结合 7.2.4 测试并发稳定性 |
| 代理支持     | 建议     | TC-GUI-010                                  | ⬜ | |
| Cookie 支持 | 建议     | TC-CLI-006, TC-GUI-010                     | ⬜ | |
| GUI 骨架    | 必测     | TC-GUI-001 ~ TC-GUI-016                     | ⬜ | GUI 能正常启动、关闭、执行基本操作 |
| UI 国际化    | 可选     | TC-GUI-015                                  | ⬜ | 只在多语言环境下重点验证 |
| 主题系统     | 可选     | TC-GUI-016                                  | ⬜ | 仅在发布前做一次外观检查即可 |

### 5.2 P1 功能测试清单

> **优先级说明：**
> - **必测**：非常常见的使用方式，实际优先级接近 P0，发布前建议至少跑一轮相关用例
> - **建议**：按重要程度标为建议，推荐有时间时执行

| 功能点       | 优先级 | 测试用例                                                    | 状态 | 备注 |
|--------------|--------|-------------------------------------------------------------|------|------|
| 双语字幕     | 建议   | TC-CLI-002, TC-GUI-002                                     | ⬜ | 检查输出目录中的双语字幕文件 |
| URL 列表模式 | 必测   | TC-CLI-004, TC-CLI-005, TC-GUI-004 ~ TC-GUI-007            | ⬜ | 这是常用模式，发布前建议必测一次 |
| 增量高级选项 | 建议   | TC-CLI-003, TC-GUI-003, TC-GUI-011, TC-GUI-012             | ⬜ | 包括「只处理新视频」等高级开关 |
| 错误分类     | 建议   | TC-CLI-007, TC-CLI-008, TC-GUI-002                         | ⬜ | 检查错误分类统计是否正确 |
| 重试策略     | 建议   | TC-CLI-002, TC-GUI-002                                     | ⬜ | 检查重试配置是否生效（尤其是网络波动场景） |
| 进度展示     | 建议   | TC-CLI-002, TC-CLI-005, TC-GUI-002, TC-GUI-007             | ⬜ | 检查进度、当前任务、ETA 展示是否合理 |
| 日志视图     | 建议   | TC-GUI-013, TC-GUI-014                                     | ⬜ | 与 5.1 的「日志系统」搭配使用，方便排查问题 |


---

## 6. 测试用例详细说明

### 6.1 输出目录结构验证

**验证点**:
- [ ] `out/` 目录存在
- [ ] `out/original/` 目录存在
- [ ] `out/translated/` 目录存在
- [ ] `out/summary/` 目录存在
- [ ] `out/metadata/` 目录存在
- [ ] 每个视频有独立的子目录
- [ ] 文件命名符合规范（语言代码）

**示例结构**:
```
out/
├── original/
│   └── <video_id>/
│       └── <lang>.srt
├── translated/
│   └── <video_id>/
│       └── <target_lang>.srt
├── summary/
│   └── <video_id>/
│       └── <summary_lang>.txt
├── metadata/
│   └── <video_id>/
│       └── metadata.json
└── bilingual/
    └── <video_id>/
        └── bilingual.<source>-<target>.srt
```

---

### 6.2 双语字幕验证

**验证点**:
- [ ] 双语字幕文件存在（如配置启用）
- [ ] 文件命名格式正确：`bilingual.<source>-<target>.srt`
- [ ] 内容格式正确（原文 + 翻译交替显示）
- [ ] 时间轴同步正确

---

### 6.3 增量记录验证

**验证点**:
- [ ] Archive 文件位置正确（用户数据目录/archives/）
- [ ] 频道模式使用频道 Archive
- [ ] URL 列表模式使用批次 Archive
- [ ] Archive 格式符合 yt-dlp 规范
- [ ] 强制重跑时正确忽略 Archive

---

### 6.4 错误处理验证

**验证点**:
- [ ] 错误被正确分类（网络、限流、内容、认证等）
- [ ] 错误信息清晰易懂
- [ ] 失败记录写入 `failed_detail.log`
- [ ] 失败 URL 写入 `failed_urls.txt`
- [ ] 错误分类统计正确显示

---

### 6.5 进度展示验证

**验证点**:
- [ ] 总进度正确显示（X/Y）
- [ ] 当前正在处理的视频列表显示
- [ ] ETA 计算合理（完成一定数量后显示）
- [ ] 统计信息实时更新
- [ ] 完成时显示最终统计和错误分类

---

## 7. v2.1 回归测试清单（AI 层与流水线）

> 本章节专门用于验证 `ide_修复任务表_AI层与流水线.md（v2.1 最终修订版）` 中 R0 / R1 / 部分 R2 任务落地后的整体稳定性。  
> 测试重点覆盖：  
> - 新 LLM 架构（LLMClient + OpenAICompatibleClient/GeminiClient/AnthropicClient）  
> - Dry Run 行为、增量处理、并发安全、临时目录清理  
> - 任务取消、AI 并发限流、Cookie 缓存等关键体验点  
>
> **优先级约定：**
> - **必测**：每个版本发布前必须至少跑一次。
> - **强烈建议**：时间允许时建议作为必测执行，尤其是大改之后。
> - **可选 / 按需**：按功能是否改动、时间情况来决定是否执行。

### 7.1 测试范围与前置条件（必测）

**测试范围：**

- 已完成或正在完成的任务：
  - R0-1 ~ R0-5（红线级，相关测试均为**必测**）
  - R1-1 ~ R1-4（高优先级，相关测试为**强烈建议**，推荐当必测用）
  - R2-1（体验优化，相关测试为**可选 / 按需**）

**前置条件：**

- 已按 `requirements.txt` 安装依赖。
- 能通过 CLI 或 GUI 正常启动工具。
- 手头有三类测试目标：
  1. **小频道 / URL 列表**：5–10 个视频（用于快速冒烟）
  2. **中等频道**：50–100 个视频（用于端到端回归）
  3. **大频道**：100–500 个视频（用于并发与取消测试）

---

### 7.2 R0 红线级回归测试（全部为「必测」）

> **说明：**  
> 本小节所有测试用例，属于 R0 红线级，对稳定性影响最大。  
> **每次重大改动合并后，至少要完整跑一遍 7.2.1 ~ 7.2.5。**

#### 7.2.1 R0-1：AI 架构 + LLM 参数（基础可用性，必测）

**目标：**

- 验证新的 LLM 架构（LLMClient + OpenAICompatibleClient 等）打通整条流水线；
- 验证 `process_video_list` 使用 `translation_llm + summary_llm` 两个参数不会再产生 NameError。

**测试步骤（CLI 示例）：**

1. **（必测）** 准备一个包含 5–10 个视频的频道或 URL 列表。
2. **（必测）** 使用同一个模型同时作为翻译和摘要模型运行一次：

   ```bash
   python cli.py channel --url "<小频道URL>" --provider openai --model gpt-4o-mini
   ```
3. **（可选增强）** 配置"翻译模型 A + 摘要模型 B"，再运行一次，用于验证双 LLM 配置（例如翻译 DeepSeek + 摘要 Gemini）。

**通过标准：**

- 不出现 NameError: translation_llm / summary_llm 等参数错误。
- 所有视频均能完成"检测 → 下载 → 翻译 → 摘要"流程。
- 日志中清晰打印所使用的 Provider/Model（用于后续追踪）。

7.3 R1：可控性（取消任务 & 并发限流，强烈建议）
说明：

R1 属于高优先级体验 / 稳定性增强。

在时间允许的情况下，建议也视为“必测”。

如果这两个功能在本轮改动中动得比较多，务必跑一遍。

7.3.1 R1-2：CancelToken 任务取消（强烈建议）
目标：

验证长时间任务在不同阶段都可以被用户中途取消，且终止过程可控、不会留脏数据。

测试步骤（强烈建议执行）：

选择一个包含 100+ 视频的大频道。

依次在不同阶段触发取消：

阶段 A：下载阶段中途取消

阶段 B：翻译阶段中途取消

阶段 C：摘要阶段中途取消

在 GUI 中触发“停止任务”按钮，或在 CLI 中通过 Ctrl+C / 特定命令触发 CancelToken。

通过标准：

 每次取消都在合理时间内响应，任务不会无限挂起。

 取消后不会留下异常大量的临时文件或半截输出。

 日志中有明确“任务被用户取消”的记录，并带有对应的上下文信息（任务 ID 或频道 URL）。

7.3.2 R1-4：AI 并发限流（不打爆 Provider，强烈建议）
目标：

验证在高下载并发的前提下，AI 调用并发仍受 LLMClient 内部 Semaphore 限制，避免杀死 DeepSeek/Gemini/Ollama 等。

测试步骤（强烈建议执行）：

配置：

下载并发：10

AI 并发（max_concurrency）：2 或 3

使用一个包含 50+ 视频的频道，执行完整任务。

观察：

AI Provider 的 QPS / 调用情况（可通过日志间接观察）。

是否出现大规模限流 / 429 / 超时错误。

通过标准：

 日志中可以看到下载任务并发较高，但 AI 调用的“并发峰值”不会超过配置的 max_concurrency。

 没有出现因为瞬时高并发导致的大面积 429 / 超时。

 偶发错误会被正常重试或记录为失败 URL，不会导致整个任务崩溃。

### 7.4 R2：体验与配置相关测试（可选 / 按需）

> **说明：**  
> R2 主要是体验相关优化。  
> 建议在有精力时补测，或者每完成一批 R2 任务时，顺手跑对应小节。

#### 7.4.1 R2-1：结构化失败记录（可选 / 按需）

**目标：**

- 验证 `failed_records.json` 文件正确生成，包含所有必需字段。

**测试步骤：**

1. **（可选）** 运行一次包含失败场景的处理任务（例如使用无效 URL 或网络错误）。
2. **（可选）** 检查 `out/failed_records.json` 文件是否存在。
3. **（可选）** 验证 JSON 记录格式：
   - 每行一个 JSON 对象（JSONL 格式）
   - 包含必需字段：`video_id`, `url`, `stage`, `error_type`, `timestamp`
   - 可选字段：`run_id`, `reason`, `channel_id`, `channel_name`
4. **（可选）** 验证 JSON 记录与 `failed_detail.log` 和 `failed_urls.txt` 的一致性。

**通过标准：**

- `failed_records.json` 文件正确生成。
- JSON 记录格式正确，字段完整。
- 记录与日志文件一致。

---

#### 7.4.2 R2-1：Cookie 测试结果缓存（可选）

**目标：**

- 验证 Cookie 测试成功后地区信息会被缓存到 config；
- 下次启动时优先读取缓存，并允许用户主动重新测试。

**测试步骤：**

1. **（可选）** 打开 GUI 或通过 CLI 启动一次 Cookie 测试流程。
2. **（可选）** 测试成功后：
   - 检查配置文件（例如 `config.json`）中是否新增地区相关字段（如 `network_region`）。
3. **（可选）** 重启应用：
   - 观察启动时是否展示当前地区信息（如"当前地区：US（缓存）"）。
   - 主动点击"重新测试地区"，确认可以重新探测并更新缓存。

**通过标准：**

- Cookie 测试成功后，配置中持久化记录地区信息。
- 下次启动时优先展示缓存结果，不会每次强制重新测试。
- 用户可以通过 UI/CLI 手动触发重新测试，并更新配置。

7.5 执行建议与记录方式（建议执行）
建议为每个小节保留一份简单的测试记录表（可存为 docs/test_reports/v2.1_xxx.md），内容包括：

测试日期

测试人

使用的频道/URL 列表

是否通过（如有问题，附上日志片段与复现步骤）

对于 R0/R1 测试，至少保证每种场景有一次完整执行记录：

小频道冒烟（5–10 视频）

中等频道回归（50–100 视频）

大频道压力 + 取消（100–500 视频）

完成本节所有「R0 必测」用例，并尽量覆盖 R1 的“强烈建议”用例后，可认为 v2.1 在 AI 层与流水线方面达到了“可长期使用”的稳定程度。

---

## 8. 测试报告模板

### 测试报告格式

```markdown
# 测试报告

**测试日期**: YYYY-MM-DD  
**测试人员**: [姓名]  
**测试版本**: v2.0  
**测试环境**: [环境描述]

## 测试概要

- **总测试用例数**: XX
- **通过**: XX
- **失败**: XX
- **跳过**: XX
- **通过率**: XX%

## 测试结果汇总

### CLI 功能测试
- [ ] 频道模式 Dry Run
- [ ] 频道模式完整处理
- [ ] URL 列表模式 Dry Run
- [ ] URL 列表模式完整处理
- [ ] Cookie 测试
- [ ] 错误处理

### GUI 功能测试
- [ ] 频道模式 Dry Run
- [ ] 频道模式完整处理
- [ ] URL 列表模式（输入、去重、导入、处理）
- [ ] 配置管理
- [ ] 增量管理
- [ ] 日志视图
- [ ] UI 国际化
- [ ] 主题切换

## 发现的问题

[使用问题记录模板记录所有问题]

## 测试结论

[总结测试结果，给出是否通过的建议]

## 建议

[如有改进建议，请列出]
```

---

## 9. 快速测试检查表

### 9.1 基础功能检查（5 分钟）

- [ ] GUI 可以正常启动
- [ ] CLI 命令可以正常执行
- [ ] 日志面板正常显示
- [ ] 配置可以保存和加载

### 9.2 核心功能检查（15 分钟）

- [ ] 频道模式 Dry Run 正常
- [ ] URL 列表模式 Dry Run 正常
- [ ] 字幕检测结果准确
- [ ] 输出目录结构正确

### 9.3 完整流程检查（30 分钟）

- [ ] 完整处理流程正常
- [ ] 进度信息正确显示
- [ ] 错误处理正常
- [ ] 增量记录正确更新

---

## 10. 测试注意事项

1. **测试数据准备**
   - 使用小的测试频道（< 10 个视频）
   - 确保测试视频有字幕
   - 准备多个测试场景（有字幕/无字幕、不同语言等）

2. **测试环境隔离**
   - 使用独立的测试配置
   - 避免影响生产数据
   - 测试后清理输出目录

3. **错误场景测试**
   - 测试网络错误
   - 测试无效 URL
   - 测试无字幕视频
   - 测试 AI API 限流

4. **性能测试**
   - 测试并发处理
   - 测试大量视频处理
   - 测试长时间运行稳定性

5. **兼容性测试**
   - 测试不同操作系统
   - 测试不同 Python 版本
   - 测试不同 yt-dlp 版本

---

## 附录

### A. 测试数据示例

**测试频道 URL**:
```
https://www.youtube.com/@testchannel
```

**测试 URL 列表** (`test_urls.txt`):
```
https://www.youtube.com/watch?v=xxxxx
https://www.youtube.com/watch?v=yyyyy
https://www.youtube.com/watch?v=zzzzz
```

### B. 常用命令参考

```bash
# CLI 频道模式 Dry Run
python cli.py channel --url <URL> --dry-run

# CLI 频道模式完整处理
python cli.py channel --url <URL> --run

# CLI URL 列表模式
python cli.py urls --file urls.txt --run

# CLI Cookie 测试
python cli.py test-cookie

# GUI 启动
python main.py
```

### C. 配置文件位置

- **Windows**: `%APPDATA%\yt-subtitle-v2\`
- **macOS**: `~/Library/Application Support/yt-subtitle-v2/`
- **Linux**: `~/.local/share/yt-subtitle-v2/`

---

**文档结束**

